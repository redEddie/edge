# Keras (Tensorflow) Alexnet 실험 결과

env: Jetson Nano - Jetpack 4.6.5, L4T 32.7.5, Tensorflow 2.7.0+nv22.1

## CPU 사용
```
INFERENCE USING CPU
THREADS = 4
avg: 18.469394421577455
max: 20.90684175491333
min: 17.215129613876343

THREADS = 3
avg: 19.740710926055907
max: 20.3710196018219
min: 19.516901969909668

THREADS = 2
avg: 25.1087525844574
max: 25.710724592208862
min: 24.978893995285034

THREADS = 1
avg: 45.64009289741516
max: 46.372063636779785
min: 45.46632671356201

THREADS = 128
avg: 20.121337628364564
max: 21.555009126663208
min: 19.35483717918396
```
스레드 개수가 1~3으로 늘어남에 따라 성능 개선이 확실히 드러남.

### 의문점
스레드 개수 3~4에서는 왜 성능 개선이 잘 보이지 않는가?  
교수님께서 진행했던 실험 환경은 왜 스레드 개수 증가에서 성능 개선을 확인할 수 없었나?

## GPU 사용
```
INFERENCE USING GPU
THREADS = 4
avg: 5.266451811790466
max: 6.79758620262146
min: 5.0010316371917725

THREADS = 3
avg: 5.651442146301269
max: 7.414099454879761
min: 5.077334880828857

THREADS = 2
avg: 5.085526752471924
max: 5.99565863609314
min: 4.911487340927124

THREADS = 1
avg: 5.463624262809754
max: 8.221438646316528
min: 5.094004392623901

THREADS = 128
avg: 5.806948328018189
max: 7.765199184417725
min: 5.509004592895508
```

GPU 사용 시에는 예상했던 대로 스레드 수와 추론 시간이 큰 관계가 없음.

### 의문점
```python
if USE_GPU:
    model.predict(x_test) # pre-prediction process to load cuda libs
```
GPU 사용 시에는 이렇게 시간 측정 전에 전체 테스트 데이터셋에 대해 미리 추론을 진행하는데도 첫 번째 추론 시간이 이어지는 추론들보다 유의미한 정도로 시간이 더 걸림. 왜?